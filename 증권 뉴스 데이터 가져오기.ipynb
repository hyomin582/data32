{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hyomin\\anaconda3\\envs\\data32\\lib\\site-packages (from requests) (2022.6.15)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp310-cp310-win32.whl (89 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, requests\n",
      "Successfully installed charset-normalizer-3.1.0 idna-3.4 requests-2.31.0 urllib3-2.0.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종목 뉴스리스트 페이지별로 전체 html코드를 긁어온 후, 리스트 형식으로 반환\n",
    "def getData(codeList, newsPage):\n",
    "    pageList = []\n",
    "    for code in codeList:\n",
    "        for page in range(newsPage, newsPage+1):\n",
    "            newsListPageUrl = 'https://finance.naver.com/item/news_news.naver?code='+str(code)+'&page='+str(page)+'&sm=title_entity_id.basic&clusterId='\n",
    "            data = requests.get(newsListPageUrl)\n",
    "            soup = BeautifulSoup(data.content.decode('euc-kr', 'replace'), 'html.parser')\n",
    "            dataFilter = soup.find('tbody')\n",
    "            pageList.append(dataFilter)\n",
    "    return pageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스페이지별로 전체 html코드를 가공하여 각 뉴스기사들의 url주소를 만들어서, 리스트 형식으로 반환\n",
    "def change(bs4Data, num):\n",
    "    makeUrlList = []\n",
    "    newsUrlList = []\n",
    "    for bs4 in bs4Data:\n",
    "        change = bs4.select('tbody > tr > td > a')\n",
    "        for i in range(len(change)):\n",
    "            if num <= 9:\n",
    "                newsUrlList.append('https://finance.naver.com'+str(change[i])[21:65]+str(change[i])[69:83]+str(change[i])[87:99]+str(change[i])[103:110]+str(change[i])[114:138])\n",
    "            elif num <= 99:\n",
    "                newsUrlList.append('https://finance.naver.com'+str(change[i])[21:65]+str(change[i])[69:83]+str(change[i])[87:99]+str(change[i])[103:111]+str(change[i])[115:139])\n",
    "            elif num <= 999:\n",
    "                newsUrlList.append('https://finance.naver.com'+str(change[i])[21:65]+str(change[i])[69:83]+str(change[i])[87:99]+str(change[i])[103:112]+str(change[i])[116:140])\n",
    "    return newsUrlList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewsData(urlLists):\n",
    "    newsDateList = []\n",
    "    newsTitleList = []\n",
    "    newsTextList = []\n",
    "    for i in range(len(urlLists)):\n",
    "        html = requests.get(urlLists[i])\n",
    "        soup = BeautifulSoup(html.content.decode('euc-kr', 'replace'), 'html.parser')\n",
    "        newsDateList.append(soup.find('span',{'class':'tah p11'}).get_text())\n",
    "        newsTitleList.append(soup.find('strong', {'class':'c p15'}).get_text())\n",
    "        newsText = soup.find('div', {'class':'scr01'})\n",
    "        if newsText.find('div', {'class':'link_news'}) != None:\n",
    "            newsText.find('div', {'class':'link_news'}).decompose()\n",
    "        newsTextList.append(newsText.get_text()[1:-1])\n",
    "    newsDf = pd.DataFrame({\n",
    "        '날짜':newsDateList\n",
    "        , '제목':newsTitleList\n",
    "        , '내용':newsTextList\n",
    "    })\n",
    "    return newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filescan():\n",
    "    fileDataList = []\n",
    "    fileList = glob('./gwajae5/박효민/뉴스데이터수집/*.csv')\n",
    "    for i in range(len(fileList)):\n",
    "        fileDataList.append(pd.read_csv('./gwajae5/박효민/뉴스데이터수집/'+fileList[i][11:], encoding='utf-8'))\n",
    "    hapData = pd.concat(fileDataList)\n",
    "    hapData.drop_duplicates(['제목'], inplace=True)\n",
    "    hapData.sort_values(['날짜'], inplace=True)\n",
    "    hapData.reset_index(inplace=True)\n",
    "    hapData.drop('index', axis=1, inplace=True)\n",
    "    hapData.to_csv('./gwajae5/박효민/뉴스데이터수집/삼성바이오로직스 뉴스데이터.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile():\n",
    "    filtering = pd.read_csv('./gwajae5/박효민/뉴스데이터수집/삼성바이오로직스 뉴스데이터.csv', encoding='utf-8')\n",
    "    for i in range(0, len(filtering)):\n",
    "        filtering['날짜'][i] = filtering['날짜'][i].strip()\n",
    "        filtering['내용'][i] = str(filtering['내용'][i]).replace('\\t','')\n",
    "    filtering.to_csv('./gwajae5/박효민/뉴스데이터수집/삼성바이오로직스 뉴스데이터.csv', encoding='utf-8', index=False)\n",
    "    return pd.read_csv('./gwajae5/박효민/뉴스데이터수집/삼성바이오로직스 뉴스데이터.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockCodeList = ['207940'] # 원하는 종목\n",
    "stockNewsPage = 1        # 원하는 뉴스페이지 숫자 1 ~ 999페이지까지만 긁어오기 가능\n",
    "\n",
    "for numberCount in range(1, stockNewsPage):\n",
    "    bs4Data = getData(stockCodeList, numberCount)\n",
    "    urlList = change(bs4Data, numberCount)\n",
    "    getNewsDf = getNewsData(urlList)\n",
    "    getNewsDf.to_csv('./gwajae5/박효민/뉴스데이터수집/삼성바이오로직스 뉴스데이터'+str(numberCount)+'.csv', encoding='utf-8', index=False) # 원하는 종목 코드에 맞는 종목 이름으로 꼭 바꿔서 사용하세요.\n",
    "\n",
    "# 진행도는 여기서 확인할 수 없으니, ./newsdata/안에 있는 파일 생성되는 것을 확인하세요. -> 네이버 기사 페이지 숫자가 파일명에 붙어서 만들어집니다.\n",
    "# 예) 하이닉스 뉴스 데이터77.csv -> 하이닉스 뉴스 77페이지 뉴스 리스트 다 긁어옴\n",
    "# 네이버 봇이 자동으로 막는 문제인지 알 수 없으나, 30~50개 정도 파일을 만들어낼때마다 'Attribute error'가 뜨는데\n",
    "# 하이닉스 뉴스 데이터93.csv 까지 파일이 만들어졌다가 에러가 떳다면\n",
    "# for numberCount in range(93, stockNewsPage) <- 이 코드에서 네이버 뉴스 페이지 수 93을 \n",
    "# 입력한채로 다시 실행하면 다시 만들기가 시작됩니다.\n",
    "# 에러가 안뜰 수도 있음.\n",
    "# 테스트 결과 하이닉스 총 344페이지 뉴스 페이지 전부 가져와지는 것을 확인함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filescan()\n\u001b[0;32m      2\u001b[0m openFile()\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36mfilescan\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(fileList)):\n\u001b[0;32m      5\u001b[0m     fileDataList\u001b[39m.\u001b[39mappend(pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m./gwajae5/박효민/뉴스데이터수집/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mfileList[i][\u001b[39m11\u001b[39m:], encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m hapData \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(fileDataList)\n\u001b[0;32m      7\u001b[0m hapData\u001b[39m.\u001b[39mdrop_duplicates([\u001b[39m'\u001b[39m\u001b[39m제목\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m hapData\u001b[39m.\u001b[39msort_values([\u001b[39m'\u001b[39m\u001b[39m날짜\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    370\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    375\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    376\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    377\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    378\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    379\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    380\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    381\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    382\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Hyomin\\anaconda3\\envs\\data32\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    426\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[0;32m    428\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "filescan()\n",
    "openFile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
